                                                                                                                                                                    
{'loss': 2.7772, 'grad_norm': 0.4804404377937317, 'learning_rate': 2e-05, 'epoch': 0.01}
{'loss': 2.873, 'grad_norm': 0.5421293377876282, 'learning_rate': 4e-05, 'epoch': 0.03}
{'loss': 3.0349, 'grad_norm': 0.615736186504364, 'learning_rate': 6e-05, 'epoch': 0.04}
{'loss': 3.2615, 'grad_norm': 0.6718720197677612, 'learning_rate': 8e-05, 'epoch': 0.05}
{'loss': 3.2039, 'grad_norm': 0.6320120096206665, 'learning_rate': 0.0001, 'epoch': 0.07}
{'loss': 2.996, 'grad_norm': 0.8170644044876099, 'learning_rate': 0.00012, 'epoch': 0.08}
{'loss': 2.7951, 'grad_norm': 0.6536177396774292, 'learning_rate': 0.00014, 'epoch': 0.1}
{'loss': 2.8415, 'grad_norm': 0.7516923546791077, 'learning_rate': 0.00016, 'epoch': 0.11}
{'loss': 2.8777, 'grad_norm': 0.7917003631591797, 'learning_rate': 0.00018, 'epoch': 0.12}
{'loss': 2.7608, 'grad_norm': 0.618701696395874, 'learning_rate': 0.0002, 'epoch': 0.14}
{'loss': 2.4831, 'grad_norm': 1.4213783740997314, 'learning_rate': 0.000196, 'epoch': 0.15}
{'loss': 2.5082, 'grad_norm': 0.7334487438201904, 'learning_rate': 0.000192, 'epoch': 0.16}
{'loss': 2.2074, 'grad_norm': 0.9344269633293152, 'learning_rate': 0.000188, 'epoch': 0.18}
{'loss': 2.2728, 'grad_norm': 0.6233378648757935, 'learning_rate': 0.00018400000000000003, 'epoch': 0.19}
{'loss': 2.3278, 'grad_norm': 0.6397780179977417, 'learning_rate': 0.00018, 'epoch': 0.2}
{'loss': 2.4534, 'grad_norm': 0.576148271560669, 'learning_rate': 0.00017600000000000002, 'epoch': 0.22}
{'loss': 2.3313, 'grad_norm': 0.6329861283302307, 'learning_rate': 0.000172, 'epoch': 0.23}
{'loss': 2.4755, 'grad_norm': 0.5991228818893433, 'learning_rate': 0.000168, 'epoch': 0.24}
{'loss': 2.4319, 'grad_norm': 0.5907768607139587, 'learning_rate': 0.000164, 'epoch': 0.26}
{'loss': 2.368, 'grad_norm': 0.5954539179801941, 'learning_rate': 0.00016, 'epoch': 0.27}
{'loss': 2.2152, 'grad_norm': 0.7254619598388672, 'learning_rate': 0.00015600000000000002, 'epoch': 0.29}
{'loss': 2.3071, 'grad_norm': 0.7319454550743103, 'learning_rate': 0.000152, 'epoch': 0.3}
{'loss': 2.4132, 'grad_norm': 0.5869263410568237, 'learning_rate': 0.000148, 'epoch': 0.31}
{'loss': 2.1098, 'grad_norm': 0.6415525674819946, 'learning_rate': 0.000144, 'epoch': 0.33}
{'loss': 2.3778, 'grad_norm': 0.7200495600700378, 'learning_rate': 0.00014, 'epoch': 0.34}
{'loss': 2.2923, 'grad_norm': 0.6281614303588867, 'learning_rate': 0.00013600000000000003, 'epoch': 0.35}
{'loss': 2.3384, 'grad_norm': 0.6447093486785889, 'learning_rate': 0.000132, 'epoch': 0.37}
{'loss': 2.3135, 'grad_norm': 0.71962571144104, 'learning_rate': 0.00012800000000000002, 'epoch': 0.38}
{'loss': 2.3188, 'grad_norm': 0.6847898364067078, 'learning_rate': 0.000124, 'epoch': 0.39}
{'loss': 2.2585, 'grad_norm': 0.6792106032371521, 'learning_rate': 0.00012, 'epoch': 0.41}
{'loss': 2.2608, 'grad_norm': 0.5915737748146057, 'learning_rate': 0.000116, 'epoch': 0.42}
{'loss': 2.1551, 'grad_norm': 0.7494463920593262, 'learning_rate': 0.00011200000000000001, 'epoch': 0.44}
{'loss': 2.2829, 'grad_norm': 0.5851847529411316, 'learning_rate': 0.00010800000000000001, 'epoch': 0.45}
{'loss': 2.2854, 'grad_norm': 0.6185713410377502, 'learning_rate': 0.00010400000000000001, 'epoch': 0.46}
{'loss': 2.2714, 'grad_norm': 0.6802932024002075, 'learning_rate': 0.0001, 'epoch': 0.48}
{'loss': 2.3891, 'grad_norm': 0.5746790170669556, 'learning_rate': 9.6e-05, 'epoch': 0.49}
{'loss': 2.3949, 'grad_norm': 0.631866991519928, 'learning_rate': 9.200000000000001e-05, 'epoch': 0.5}
{'loss': 2.2147, 'grad_norm': 0.633033275604248, 'learning_rate': 8.800000000000001e-05, 'epoch': 0.52}
{'loss': 2.2718, 'grad_norm': 0.614220917224884, 'learning_rate': 8.4e-05, 'epoch': 0.53}
{'loss': 2.4007, 'grad_norm': 0.6281097531318665, 'learning_rate': 8e-05, 'epoch': 0.54}
{'loss': 2.3569, 'grad_norm': 0.6032353639602661, 'learning_rate': 7.6e-05, 'epoch': 0.56}
{'loss': 2.2189, 'grad_norm': 0.6019800305366516, 'learning_rate': 7.2e-05, 'epoch': 0.57}
{'loss': 2.1685, 'grad_norm': 0.7025164365768433, 'learning_rate': 6.800000000000001e-05, 'epoch': 0.59}
{'loss': 2.2641, 'grad_norm': 0.5831881761550903, 'learning_rate': 6.400000000000001e-05, 'epoch': 0.6}
{'loss': 2.1953, 'grad_norm': 0.7098804712295532, 'learning_rate': 6e-05, 'epoch': 0.61}
{'loss': 2.0795, 'grad_norm': 0.7305384278297424, 'learning_rate': 5.6000000000000006e-05, 'epoch': 0.63}
{'loss': 2.2868, 'grad_norm': 0.6957300901412964, 'learning_rate': 5.2000000000000004e-05, 'epoch': 0.64}
{'loss': 2.2646, 'grad_norm': 0.5466232299804688, 'learning_rate': 4.8e-05, 'epoch': 0.65}
{'loss': 2.0149, 'grad_norm': 0.6336444020271301, 'learning_rate': 4.4000000000000006e-05, 'epoch': 0.67}
{'loss': 2.059, 'grad_norm': 0.6620819568634033, 'learning_rate': 4e-05, 'epoch': 0.68}
{'loss': 2.3511, 'grad_norm': 0.7149783372879028, 'learning_rate': 3.6e-05, 'epoch': 0.69}
{'loss': 2.2157, 'grad_norm': 0.5747998356819153, 'learning_rate': 3.2000000000000005e-05, 'epoch': 0.71}
{'loss': 2.2119, 'grad_norm': 0.6502789258956909, 'learning_rate': 2.8000000000000003e-05, 'epoch': 0.72}
{'loss': 2.3704, 'grad_norm': 0.6406059265136719, 'learning_rate': 2.4e-05, 'epoch': 0.73}
{'loss': 1.9625, 'grad_norm': 0.6640687584877014, 'learning_rate': 2e-05, 'epoch': 0.75}
{'loss': 2.2063, 'grad_norm': 0.689489483833313, 'learning_rate': 1.6000000000000003e-05, 'epoch': 0.76}
{'loss': 2.0961, 'grad_norm': 0.5766138434410095, 'learning_rate': 1.2e-05, 'epoch': 0.78}
{'loss': 2.0964, 'grad_norm': 0.5794379115104675, 'learning_rate': 8.000000000000001e-06, 'epoch': 0.79}
{'loss': 2.2685, 'grad_norm': 0.590471088886261, 'learning_rate': 4.000000000000001e-06, 'epoch': 0.8}
{'loss': 2.274, 'grad_norm': 0.7332973480224609, 'learning_rate': 0.0, 'epoch': 0.82}
  warnings.warn(
/home/ubuntu/.local/lib/python3.12/site-packages/peft/utils/save_and_load.py:243: UserWarning: Could not find a config file in unsloth/meta-llama-3.1-8b-instruct-bnb-4bit - will assume that the vocabulary was not modified.
  warnings.warn(
100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [14:21<00:00, 14.36s/it]
{'train_runtime': 864.1943, 'train_samples_per_second': 0.555, 'train_steps_per_second': 0.069, 'train_loss': 2.3852320770422617, 'epoch': 0.82}
864.1943 seconds used for training.
14.4 minutes used for training.
Peak reserved memory = 8.967 GB.
Peak reserved memory for training = 2.983 GB.
Peak reserved memory % of max memory = 61.718 %.
Peak reserved memory for training % of max memory = 20.531 %.
['<|begin_of_text|>Below is an instruction that describes a task, paired with '
 'an input that provides further context. \n'
 'Write a response that appropriately completes the request.\n'
 '\n'
 '### Instruction:\n'
 'Write post about the following topic: \n'
 '\n'
 '### Input:\n'
 '–ü–ª–∞–Ω—ã –Ω–∞ –Ω–æ–≤—ã–π –≥–æ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ Data Science\n'
 '\n'
 '### Response:\n'
 'ü•≥ –í—Å—Ç—Ä–µ—á–∞ —Å –Ω–æ–≤—ã–º –≥–æ–¥–æ–º!\n'
 '\n'
 'üéÅ –ü–ª–∞–Ω—ã –Ω–∞ –Ω–æ–≤—ã–π –≥–æ–¥ –≤ –æ–±–ª–∞—Å—Ç–∏ Data Science. \n'
 '\n'
 'ü§î –ö–∞–∫–∏–µ —É –≤–∞—Å –ø–ª–∞–Ω—ã –Ω–∞ –Ω–æ–≤—ã–π –≥–æ–¥? \n'
 '\n'
 'üéÖ –ö–∞–∫–æ–π –ø–ª–∞–Ω –≤—ã –±—ã –≤—ã–±—Ä–∞–ª–∏, –µ—Å–ª–∏ –±—ã –≤–∞–º –¥–∞–ª–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤—ã–±—Ä–∞—Ç—å? \n'
 '\n'
 'ü§î –ö–∞–∫–∏–µ –∑–∞–¥–∞—á–∏ –≤—ã –±—ã —Ä–µ–∞–ª–∏–∑–æ–≤–∞–ª–∏ –≤ –±–ª–∏–∂–∞–π—à–∏–µ 6 –º–µ—Å—è—Ü–µ–≤? \n'
 '\n'
 'üéÖ –ö–∞–∫–∏–µ –Ω–æ–≤–æ–≤–≤–µ–¥–µ–Ω–∏—è –≤ –æ–±–ª–∞—Å—Ç–∏ Data Science –≤—ã –±—ã –≤–∫–ª—é—á–∏–ª–∏ –≤ —Å–≤–æ–π '
 '–ø–ª–∞–Ω?<|eot_id|>']
